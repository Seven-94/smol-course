![smolcourse image](./banner.png)

# a smol course

Il s'agit d'un cours pratique sur l'alignement des mod√®les linguistiques pour votre cas d'utilisation sp√©cifique. C'est un moyen pratique de commencer √† aligner des mod√®les de langage, car tout fonctionne sur la plupart des machines locales. Les besoins en GPU sont minimes et il n'y a pas de services payants. Le cours est bas√© sur la s√©rie de mod√®les [SmolLM2](https://github.com/huggingface/smollm/tree/main), mais vous pouvez transf√©rer les comp√©tences que vous apprenez ici √† des mod√®les plus grands ou √† d'autres petits mod√®les de langue.

<a href="http://hf.co/join/discord">
<img src="https://img.shields.io/badge/Discord-7289DA?&logo=discord&logoColor=white"/>
</a>

<div style="background: linear-gradient(to right, #e0f7fa, #e1bee7, orange); padding: 20px; border-radius: 5px; margin-bottom: 20px; color: purple;">
    <h2>La participation est ouverte, gratuite et imm√©diate!</h2>
    <p>Ce cours est ouvert et √©valu√© par les pairs. Pour participer au cours <strong>ouvrir une demande de pull request</strong> t soumettez votre travail pour examen. Voici la marche √† suivre :</p>
    <ol>
        <li>Fork le repo <a href="https://github.com/huggingface/smol-course/fork">ici</a></li>
        <li>Lisez le mat√©riel, apportez des modifications, faites les exercices, ajoutez vos propres exemples.</li>
        <li>Ouvre un PR dans la branche december_2024</li>
        <li>Fais le r√©viser et fusionner (merged)</li>
    </ol>
    <p>Cela devrait t'aider √† apprendre et √† construire un cours ax√© sur la communaut√© qui s'am√©liore constamment.</p>
</div>

Nous pouvons discuter de ce processus dans ce document. [discussion thread](https://github.com/huggingface/smol-course/discussions/2#discussion-7602932).

## Plan du cours

Ce cours propose une approche pratique de l'utilisation de petits mod√®les linguistiques, depuis la formation initiale jusqu'au d√©ploiement en production.

| Module | Description | Etat | Date de sortie |
|--------|-------------|---------|--------------|
| [Instruction Tuning](./1_instruction_tuning) | Apprendre les r√©glages fins supervis√©s (supervised fine-tuning), la cr√©ation de mod√®les de chat (chat templating) et les instructions de base qui suivent. | ‚úÖ Complet | Dec 3, 2024 |
| [Preference Alignment](./2_preference_alignment) | Explorer les techniques DPO et ORPO pour aligner les mod√®les sur les pr√©f√©rences humaines | üöß En Construction  | Dec 6, 2024 |
| [Parameter-efficient Fine-tuning](./3_parameter_efficient_finetuning) | Apprendre le LoRA, le r√©glage rapide et les m√©thodes d'adaptation efficaces | üöß En Construction | Dec 9, 2024 |
| [Evaluation](./4_evaluation) | Utiliser des crit√®res de r√©f√©rence automatiques et cr√©er des √©valuations de domaine personnalis√©es | üöß En Construction | Dec 16, 2024 |
| [Vision-language Models](./5_vision_language_models) | Adapter les mod√®les multimodaux aux t√¢ches vision-langage | üìù Pr√©vu | Dec 20, 2024 |
| [Synthetic Datasets](./6_synthetic_datasets) | Cr√©er et valider des ensembles de donn√©es synth√©tiques pour le training | üìù Pr√©vu | Dec 23, 2024 |
| [Inference](./7_inference) | Inf√©rer efficacement avec des mod√®les | üìù Pr√©vu | Dec 27, 2024 |
| [Deployment](./8_deplyment) | D√©ployer et servir des mod√®les √† grande √©chelle | üìù Pr√©vu | Dec 30, 2024 |

## Pourquoi des petits mod√®les linguistiques ?

Bien que les mod√®les linguistiques de grande taille aient montr√© des capacit√©s impressionnantes, ils n√©cessitent souvent des ressources informatiques importantes et peuvent √™tre excessifs pour des applications cibl√©es. Les petits mod√®les linguistiques offrent plusieurs avantages pour les applications sp√©cifiques √† un domaine :

- **Efficacit√©** : La formation et le d√©ploiement n√©cessitent beaucoup moins de ressources informatiques.
- **Personnalisation** : Plus facile √† affiner(fine-tune) et √† adapter √† des domaines sp√©cifiques
- **Contr√¥le** : Meilleure compr√©hension et contr√¥le du comportement du mod√®le
- **Co√ªt** : Co√ªts op√©rationnels r√©duits pour le training et l'inf√©rence
- **Confidentialit√©** : Peut √™tre ex√©cut√© localement sans envoyer de donn√©es √† des API externes
- **Technologie verte** : utilisation efficace des ressources et r√©duction de l'empreinte carbone
- **D√©veloppement plus facile de la recherche universitaire** : Facilite le d√©marrage de la recherche universitaire avec des LLM de pointe et des contraintes logistiques moindres.

## Conditions pr√©alables

Avant de commencer, assurez-vous de disposer des √©l√©ments suivants
- Compr√©hension de base de l'apprentissage automatique et du traitement du langage naturel.
- Familiarit√© avec Python, PyTorch et la biblioth√®que `transformers`.
- Acc√®s √† un mod√®le de langage pr√©-entra√Æn√© et √† un ensemble de donn√©es √©tiquet√©es.

## Installation

Nous maintenons le cours sous forme de paquetage afin que vous puissiez installer facilement les d√©pendances via un gestionnaire de paquetage. Nous recommandons [uv](https://github.com/astral-sh/uv) √† cette fin, mais vous pouvez utiliser des alternatives comme `pip` ou `pdm`.

### Utilisation de `uv`

Avec `uv` install√©, vous pouvez installer le cours comme suit:

```bash
uv venv --python 3.11.0
uv sync
```

### Utilisation de `pip`

Tous les exemples fonctionnent dans le m√™me environnement **python 3.11**, vous devez donc cr√©er un environnement et installer les d√©pendances comme ceci :

```bash
# python -m venv .venv
# source .venv/bin/activate
pip install -r requirements.txt
```

### Google Colab

**From Google Colab** vous devrez installer les d√©pendances de mani√®re flexible en fonction du mat√©riel que vous utilisez. Comme ceci :

```bash
pip install -r transformers trl datasets huggingface_hub
```

## Engagement

Partageons-le, afin que de nombreuses personnes puissent apprendre √† affiner les LLM sans mat√©riel co√ªteux.

[![Star History Chart](https://api.star-history.com/svg?repos=huggingface/smol-course&type=Date)](https://star-history.com/#huggingface/smol-course&Date)
